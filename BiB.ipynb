{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2fa9f8-284b-4f96-9c9e-e566ca4144b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a13c62-6d79-4f21-9948-a8093a3ecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_groups = [\n",
    "    # About documents\n",
    "    ['unreport', 'unreported', 'unreporting', 'unreports', 'report', ....., 'misreport',....., 'nonreport',...., 'underreport', 'underreports', 'underreported', 'underreporting'],\n",
    "    ['unrecord, record, nonrecord',...],\n",
    "    ['undocumented', 'undocument', 'undocumenting', 'undocuments'],\n",
    "    ['misrepresent', 'misrepresented', 'misrepresenting', 'misreports'],\n",
    "    ['register', 'registers', 'registered', 'registering', 'unregister', 'unregisters', 'unregistered', 'unregistering'],\n",
    "    ['logbook', 'logged'],\n",
    "    ['declaration', 'declare', 'declares', 'declared', 'declaring', 'underdeclaration', 'under-declaration'],\n",
    "    ['label', 'mislabel', 'labeling', 'labeled', 'labels', 'mislabeling', 'mislabled', 'mislables'],\n",
    "    ['evade',],\n",
    "    ['disappear',....,'reappear',...,'appear',.... ],\n",
    "    ['reveal',..., 'discover',..., 'vanish', ],\n",
    "    ['endanger',..., 'protect',..., 'restrict',...],\n",
    "    ['undisclose',..., 'disclose',.... ],\n",
    "    ['ban',..., 'restrict',...],\n",
    "    ['hide',..],\n",
    "    ['bypass',],\n",
    "    \n",
    "    # About being false\n",
    "    ['false', 'falsify', 'falsifies', 'falsifying', 'falsifies', 'falsified', 'falsification', 'fake', 'manipulated', 'manipulate', 'manipulates', 'manipulating', 'manipulation'],\n",
    "    ['fraud', 'fraudulence', 'hoax'],\n",
    "    # About amounts\n",
    "    ['volume', 'quota', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'inflation'],\n",
    "    # Fish, Ocean, port \n",
    "    ['catch', 'bycatch', 'harvest', 'juvenile', 'invasive'],\n",
    "    ['farmed', 'farming'], \n",
    "    ['ocean', 'sea', 'seas', 'water'],\n",
    "    ['port'],\n",
    "    ['transshipment'],\n",
    "    ['landing', 'trade','import'],\n",
    "    ['selective'],    \n",
    "#    ['commercial', 'industrial'],\n",
    "    # Crime\n",
    "    ['offend', 'offended', 'offends', 'violate', 'violates', 'violated', 'violation'],\n",
    "    ['regulation', 'regulate', 'regulated', 'regulates', 'regulating', 'regulations', 'regulates', unregulate,],\n",
    "    ['alter', 'altering', 'alters'],\n",
    "    ['exploit', 'exploited', 'exploiting', 'exploits'],\n",
    "    ['impose', 'imposed', 'imposes', 'imposing'],\n",
    "    ['detect', ..., 'prosecute',...]\n",
    "    ['inspector', 'inspectors', 'inspect', 'inspects', 'inspected', 'inspecting','investigate', 'investigates', 'investigated','investigating','investigator'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf286153-9196-439d-b5b2-c16fa2d4aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    " #, aggregate, , import, export, monitor, unload, offload overload load, spaw, doctor, undeclare, permit, threaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d118baa-262f-4ddb-966d-99b1825c004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the contents in the url\n",
    "#### input: url\n",
    "#### output: content (type: str)\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#print(soup)\n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "###### The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 10)):\n",
    "### For the whole content:\n",
    "#        for i in range(len(paragraphs)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "scrape_content('https://www.msn.com/en-ca/lifestyle/other/experts-urge-people-to-fish-and-eat-crab-species-putting-entire-fishing-industry-at-risk-an-animal-of-unacceptable-intelligence/ar-BB1na7wc?ocid=BingNewsSearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf20bc7c-d913-4e8c-a239-12267ce9343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from google news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates are in the form of \"1 month ago\", \"3 hours ago\"\n",
    "########################################################\n",
    "\n",
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://www.google.com/search?q={query}&tbm=nws\"\n",
    "    print(url)\n",
    "#    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='Gx5Zad fP1Qef xpd EtOod pkphOe')\n",
    "    date_elements = soup.find_all('span', class_='r0bn4c rQMQod')\n",
    "\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 5)):\n",
    "        title = search_results[i].find('a').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        #link = 'https://news.google.com' + link[1:]\n",
    "        date = date_elements[i].text   \n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc0767e-4aa4-4c47-841d-e37c2b7b8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])\n",
    "    \n",
    "queries = queries[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14ddc3f-680b-4591-a040-f3ef05fa62d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  Vessel caught misreporting catch amount\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=Vessel caught misreporting catch amount&tbm=nws\n",
      "1. Sea Shepherd GlobalSea Shepherd GlobalSea Shepherd Global stands at the forefront of the fight against Illegal, Unreported, and Unregulated (IUU) fishing, deploying innovative...2 weeks ago\n",
      "https://www.seashepherdglobal.org/latest-news/combat-iuu-fishing/&sa=U&ved=2ahUKEwjkrqPF_e-GAxXPOjQIHcb2G1UQxfQBegQIBRAC&usg=AOvVaw3Cu8PkdOYu-fqd2ueoPEmj\n",
      "2 weeks ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n",
      "2. New rules tighten controls on EUâ€¦Environmental Justice FoundationEuropean Commission approves new rules that require stricter controls for landings by EU fishing vessels.1 month ago\n",
      "https://ejfoundation.org/news-media/new-rules-tighten-controls-on-eu-vessels-to-prevent-misreporting&sa=U&ved=2ahUKEwjkrqPF_e-GAxXPOjQIHcb2G1UQxfQBegQICRAC&usg=AOvVaw2NwYoRjrYtpQI5mPXwe-dM\n",
      "1 month ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n",
      "3. N.S. boat captain, 2 companies fined $125K for fisheries violationsCBCThe case involved misreporting of halibut, hake and cod catch from trips on board the fishing boat Ivy Lew between May 2019 and June 2020.9 months ago\n",
      "https://www.cbc.ca/news/canada/nova-scotia/n-s-boat-captain-fined-fisheries-violations-1.6965198&sa=U&ved=2ahUKEwjkrqPF_e-GAxXPOjQIHcb2G1UQxfQBegQIARAC&usg=AOvVaw1wz1H2rMFbUeDLVBopzT6T\n",
      "9 months ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n",
      "4. Move the IUU Fight Up the Food Chain | Proceedings - November 2023 Vol. 149/11/1,449U.S. Naval InstituteBolstering oversight of transshipment vessels would take counterâ€“IUU fishing efforts to a new level.7 months ago\n",
      "https://www.usni.org/magazines/proceedings/2023/november/move-iuu-fight-food-chain&sa=U&ved=2ahUKEwjkrqPF_e-GAxXPOjQIHcb2G1UQxfQBegQIAxAC&usg=AOvVaw3P8l38B7dtTn-_b6uQ1Xw9\n",
      "7 months ago\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "5. Surprise! Media is misreporting the source of a Dutch cargo ship fireElectrekEarly this morning, the Fremantle Highway, a vehicle carrying cargo ship, caught fire in the North Sea, off the coast of Ameland in the...10 months ago\n",
      "https://electrek.co/2023/07/26/surprise-media-is-misreporting-the-source-of-a-dutch-cargo-ship-fire/&sa=U&ved=2ahUKEwjkrqPF_e-GAxXPOjQIHcb2G1UQxfQBegQIBhAC&usg=AOvVaw2Yv6rdM6aSKAuV6cilrgwI\n",
      "10 months ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = result['link'][7:]\n",
    "    print(link)\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bdbf19-eb2d-456e-9110-59941bbbfa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
